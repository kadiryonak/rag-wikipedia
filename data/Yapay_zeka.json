{
  "title": "Yapay zeka",
  "content": "Yapay zekâ (YZ) (İngilizce: artificial intelligence ya da kısaca AI), insanlar da dahil olmak üzere hayvanlar tarafından, doğal zekânın aksine makineler tarafından görüntülenen zekâ çeşididir. Dördüncü Sanayi Devrimi'nin en yaygın özelliklerinden biri olarak kabul edilir. İlk ve ikinci kategoriler arasındaki ayrım genellikle seçilen kısaltmayla ortaya çıkar. Güçlü yapay zeka genellikle Yapay genel zekâ (İngilizce: artificial general intelligence kelimelerinin kısaltılmışı olarak: AGI) olarak etiketlenirken, doğal zekayı taklit etme girişimleri yapay biyolojik zekâ (İngilizce: artificial biological intelligence: ABI) olarak adlandırılır. Önde gelen yapay zeka ders kitapları, alanı zeki etmenlerin çalışması olarak tanımlar: Çevresini algılayan ve hedeflerine başarıyla ulaşma şansını en üst düzeye çıkaran eylemleri gerçekleştiren herhangi bir cihaz. Halk arasında, yapay zekâ kavramı genellikle insanların insan zihni ile ilişkilendirdiği öğrenme ve problem çözme gibi bilişsel eylemleri taklit eden makineleri tanımlamak için kullanılır.\nMakineler daha becerikli hâle geldikçe, zekâ gerektirdiği düşünülen görevler genellikle YZ etkisi olarak bilinen bir fenomen olan YZ tanımından çıkarılır. Tesler'in teoremindeki bir espri, \"YZ henüz yapılmamış şeydir\" der. Örneğin, optik karakter tanıma YZ olarak değerlendirilen şeylerin dışında tutulur, rutin teknoloji hâline gelir.\nGenellikle yapay zekâ olarak sınıflandırılan modern makine yetenekleri satranç ve Go gibi stratejik oyun sistemlerinde, en üst düzeyde rekabet eden insan konuşmasını anlama, poker ya da otonom arabalar gibi kusurlu-bilgi oyunlarını içerik dağıtım ağındaki akıllı yönlendirmeyi ve askeri simülasyonları kapsar.\nYapay zekâ çalışmaları sıklıkla insanın düşünme yöntemlerini taklit eden yapay algoritmalar geliştirmeye yöneliktir, ancak bununla sınırlı değildir. Öğrenebilen ve gelecekte insan zekâsından bağımsız gelişebilecek bir yapay zekâ kavramına doğru yeni yönelimler oluşmaktadır. Bu yönelim, insanın evreni ve doğayı anlama çabasında kendisine yardımcı olabilecek belki de kendisinden daha zeki, insan ötesi varlıklar meydana getirme düşünün bir ürünüdür. Bu düş, 1920'li yıllarda yazılan ve sonraları Isaac Asimov'u etkileyen modern bilimkurgu edebiyatının öncü yazarlarından Karel Čapek'in eserlerinde dışa vurmuştur. Karel Čapek, R.U.R. adlı tiyatro oyununda yapay zekâya sahip robotlar ile insanlığın ortak toplumsal sorunlarını ele alarak 1920 yılında yapay zekânın insan aklından bağımsız gelişebileceğini öngörmüştür.\n\n\n== Tanım ==\nYapay zekâ, idealleştirilmiş bir perspektife göre insan zekâsına özgü yüksek bilişsel fonksiyonları veya otonom davranışları sergileyen bir yapay işletim sistemidir. Bu sistem, algılama, öğrenme, çoğul kavramları bağlama, düşünme, fikir yürütme (belirtme), sorun çözme, iletişim kurma ve karar verme gibi yeteneklere sahip olmalıdır. Ayrıca, bu yapay zekâ sistemi düşüncelerinden tepkiler üretebilmeli (eyleyici yapay zekâ) ve bu tepkileri fiziksel olarak dışa vurabilmelidir.\n\n\n== Hedefler ==\nZekâyı simüle etme (veya oluşturma) genel problemi, alt problemlere ayrılmıştır. Bu alt problemler, araştırmacıların zeki bir sistemden sergilemesini beklediği belirli özellikler veya yeteneklerden oluşur. Aşağıda açıklanan özellikler, yapay zekâ araştırmalarında en çok dikkat çeken konular arasında yer almakta ve bu araştırmaların kapsamını belirlemektedir.\n\n\n=== Akıl yürütme ve problem çözme ===\nYapay zekânın ilk araştırmacıları, insanların bulmacaları çözerken veya mantıksal çıkarımlar yaparken kullandığı adım adım akıl yürütmeyi taklit eden algoritmalar geliştirmiştir. 1980'lerin sonları ve 1990'larda, belirsiz veya eksik bilgiyle başa çıkmak için olasılık ve ekonomi kavramlarını kullanan yöntemler geliştirilmiştir.\nBu algoritmaların birçoğu, büyük ölçekli akıl yürütme problemlerini çözmek için yetersizdir, çünkü \"kombinatoryal patlama\" denilen bir durum yaşarlar: Problemler büyüdükçe bu algoritmaların çalışması üstel bir şekilde yavaşlar. Hatta insanlar bile erken dönem yapay zekânın modelleyebildiği adım adım akıl yürütmeyi nadiren kullanır. İnsanlar, çoğu sorunlarını hızlı ve sezgisel yargılarla çözerler. Doğru ve verimli akıl yürütme, hâlâ çözülememiş bir problemdir.\n\n\n=== Bilgi temsili ===\nBilgi temsili ve bilgi mühendisliği, yapay zekâ programlarının sorulara akıllıca yanıt vermesini ve gerçek dünya ile ilgili çıkarımlarda bulunmasını sağlar. Formel bilgi temsilleri; içerik tabanlı indeksleme ve bilgi erişimi, sahne yorumlama, klinik karar destek sistemleri, bilgi keşfi (büyük veritabanlarından \"ilginç\" ve eyleme geçirilebilir çıkarımlarda bulunma) gibi birçok alanda kullanılmaktadır.\n\n\n== Tarihçe ==\nYapay zekâ tarihi, antik çağlarda, usta zanaatkarlar tarafından zeka veya bilinç kazandırılan yapay varlıklara ilişkin mitler, hikâyeler ve söylentilerle başladı. Mekanik ya da \"formel\" akıl yürütme üzerine çalışmalar, antik çağda filozoflar ve matematikçilerle birlikte devam etti. Mantık alanındaki bu çalışmalar, Alan Turing'in algoritmalar teorisinin temelini oluşturdu. Turing'in teorisi, \"0\" ve \"1\" gibi basit sembolleri kullanarak bir makinenin, insan aklının gerçekleştirebileceği her türlü matematiksel akıl yürütmeyi simüle edebileceğini öne sürdü. Bu teori, sibernetik, bilgi teorisi ve nörobiyoloji gibi alanlardaki keşiflerle birleşerek, araştırmacılara \"elektronik bir beyin\" inşa etme olasılığını düşünmeye yöneltti. Öne çıkan çalışmalar arasında, 1943'teki Warren McCullouch ve Walter Pitts'in \"yapay nöronlar\" tasarımı ve Turing'in 1950'de yayımladığı, \"makine zekâsının\" mümkün olduğunu gösterdiği Turing testini tanıtan Bilgi İşlem Makineleri ve Zekâ adlı makalesi bulunmaktaydı. Bu dönemde yapılan bu tür araştırmalar, yapay zekânın temel alanlarının ortaya çıkardı.\n1956'da Dartmouth College'deki bir atölye çalışmasında yapay zeka araştırma alanı kuruldu. Bu atölyeye katılan araştırmacılar, 1960'larda yapay zekâ alanına liderlik ettiler. Öğrencileriyle beraber bu araştırmacılar, basının \"şaşırtıcı\" olarak nitelendirdiği projeler ürettiler. Bu dönemde bilgisayarlar, dama stratejileri öğrenmiş, cebir problemlerini çözmüş, mantıksal teoremleri kanıtlamış ve İngilizce konuşmuştu. 1950'lerin sonu ve 1960'ların başlarında, hem İngiltere'deki hem de Amerika Birleşik Devletleri'ndeki birçok üniversitede yapay zekâ laboratuvarları kuruldu.\n1960'lar ve 1970'lerdeki araştırmacılar, genel zekâya sahip bir makine üretmenin mümkün olduğuna inanıyor ve bunu alanlarının nihai hedefi olarak görüyordu. 1965'te Herbert Simon, \"makinelerin yirmi yıl içinde bir insanın yapabileceği her işi yapabilir hâle geleceği\" tahmininde bulunmuş, 1967'de Marvin Minsky, \"bir nesil içinde ... 'yapay zekâ' sorununun büyük ölçüde çözüleceği\" yorumunu yapmıştı. Ancak bu öngörüler, yapay zekânın geliştirilmesindeki zorlukları göz ardı etmişti. 1974'te Sir James Lighthill'ın eleştirileri ve ABD Kongresi'nin daha somut projelere fon sağlama baskısı nedeniyle, ABD ve İngiltere hükûmetleri keşif amaçlı yapay zekâ araştırmalarına verilen desteği durdurma kararı aldı. Minsky ve Seymour Papert'ın Perceptrons adlı kitabı, yapay sinir ağlarının gerçek dünya problemlerini çözmede etkisiz olduğunu savunmuş ve bu yaklaşım, itibarını kaybetmişti. Bunun sonucunda, yapay zekâ projelerine fon bulmanın zorlaştığı \"yapay zekâ kışı\" adı verilen bir dönem başladı.\n1980'lerin başında yapay zekâ araştırmaları, uzman sistemlerin ticari başarısıyla yeniden canlandı. Uzman sistemler, uzmanların bilgi ve analiz yeteneklerini taklit eden bir tür yapay zekâ programıydı. 1985 yılına gelindiğinde, yapay zekâ pazarı 1 milyar doları aşmıştı. Aynı dönemde Japonya'nın beşinci nesil bilgisayar projesi, ABD ve Britanya hükûmetlerini akademik araştırmalara yeniden fon sağlamaya teşvik etti. Ancak, 1987'de Lisp makinesi pazarının çöküşüyle yapay zekâ tekrar itibar kaybetti ve daha uzun süren ikinci bir \"yapay zekâ kışı\" başladı.\nO zamana kadar yapay zekâ projelerinin büyük kısmı, planlar, hedefler, inançlar ve bilinen gerçekler gibi zihinsel kavramları temsil etmek için yüksek seviyede semboller kullanmaya odaklanmıştı. Ancak 1980'lerde bazı araştırmacılar, bu yaklaşımın insan bilişinin tüm süreçlerini, özellikle algı, robotik, öğrenme ve örüntü tanıma gibi karmaşık süreçleri taklit edemeyeceği konusunda şüphe duymaya başladı. Bu nedenle, \"alt sembolik\" yöntemlere yöneldiler. Rodney Brooks, genel olarak \"temsil\" fikrini reddederek hayatta kalabilen ve hareket edebilen makineler geliştirmeye odaklandı. Judea Pearl ve Lofti Zadeh gibi isimler, eksik ve belirsiz bilgileri mantıksal kesinlik yerine mâkul tahminlerle işleyebilen yöntemler geliştirdi. Ayrıca Geoffrey Hinton ve diğer araştırmacılar, \"bağlantısallık\" ve sinir ağ araştırmalarını yeniden canlandırarak bu alanın gelişimine katkıda bulundu. 1990'da Yann LeCun, evrişimli sinir ağlarının el yazısı ile yazılmış rakamları tanıyabildiğini gösterdi; bu, sinir ağlarının birçok başarılı uygulamasından ilkiydi.\nYapay zekâ, 1990'ların sonları ve 21. yüzyılın başlarında, formal matematik yöntemlerinden yararlanarak ve belirli problemlere özel çözümler geliştirerek itibarını yeniden kazanmaya başladı. Bu \"dar\" ve \"formal\" yaklaşım, araştırmacıların doğrulanabilir sonuçlar üretmesini ve istatistik, ekonomi ve matematik gibi diğer alanlarla iş birliği yapmasını sağladı. 2000'li yıllara gelindiğinde, yapay zekâ araştırmacılarının geliştirdiği çözümler yaygın olarak kullanılmaya başlanmıştı. Ancak 1990'larda bu çözümler genellikle \"yapay zekâ\" olarak adlandırılmıyordu (bu eğilim yapay zekâ etkisi olarak bilinir).\nBuna karşın, bazı akademisyenler yapay zekânın ilk hedefinden, yani çok yönlü ve tam anlamıyla zeki makineler yaratma amacından uzaklaştığını düşünmeye başladı. 2002'nin başlarında, bu hedefi yeniden canlandırmak amacıyla yapay genel zekâ (\"AGI\") adı verilen bir alt alan kuruldu. 2010'lara gelindiğinde, yapay genel zekâ araştırmaları için iyi finanse edilen birçok kurum ortaya çıktı.\nDerin öğrenme, 2012'de endüstri standartlarında önemli bir üstünlük sağlayarak yapay zekâ alanında yaygın bir şekilde benimsendi.\nBirçok spesifik görevde diğer yöntemler terk edildi.\nBu başarının arkasında daha hızlı bilgisayarlar, grafik işlem birimleri, bulut bilişim gibi donanım geliştirmeleriyle ImageNet gibi dikkatle oluşturulmuş veri kümelerinin de dâhil olduğu büyük miktarda veriye erişim yatıyordu. Derin öğrenmenin bu yükselişi, yapay zekâya olan ilgi ve finansmanda büyük bir artışı tetikledi. 2015-2019 yılları arasında makine öğrenimi araştırmalarındaki toplam yayımlanma sayısında %50'lik bir artış görüldü.\n2016'da, adalet ve teknolojinin yanlış kullanımı gibi etik sorunlar, makine öğrenimi konferanslarında ana gündem maddesi hâline geldi. Bu konularda yapılan yayımlar hızla artarken, yeni finansman kaynakları sağlandı ve birçok araştırmacı kariyerlerini bu alanlara yönlendirdi. Aynı dönemde, yapay zekânın insan çıkarlarına uygun şekilde geliştirilmesini konu alan \"uyum problemi\" akademik bir çalışma alanı olarak ciddiyet kazandı.\n2010'ların sonları ve 2020'lerin başlarında, yapay genel zekâ üzerine çalışan şirketler dikkat çeken yazılımlar üretmeye başladı. 2015 yılında DeepMind tarafından geliştirilen AlphaGo, oyunun kuralları öğretilerek kendi stratejisini geliştirdi ve dünya şampiyonu Go oyuncusunu yendi. 2020'de OpenAI tarafından çıkarılan GPT-3, insan benzeri yüksek kaliteli metinler üretebilen bir geniş dil modeli olarak dikkat çekti. 30 Kasım 2022'de kullanıma sunulan ChatGPT ise iki ay içinde 100 milyon kullanıcıya ulaşarak tarihin en hızlı büyüyen tüketici yazılımı oldu. 2022 yılı, yapay zekânın geniş kitleler tarafından fark edildiği ve yaygın bir şekilde konuşulmaya başlandığı bir dönüm noktası olarak kabul edildi. Bu yazılımlar, büyük bir yapay zeka patlamasını tetikledi. Büyük ölçekli şirketler yapay zekâ araştırmalarına milyarlarca dolar yatırım yapmaya başladı. AI Impacts'in verilerine göre, 2022 yılında sadece ABD'de yapay zekâya yıllık yaklaşık $50 milyar yatırım yapıldı. ABD'deki yeni bilgisayar bilimi doktora mezunlarının yaklaşık %20'si yapay zekâ üzerine uzmanlaştı. Aynı yıl, ABD'de yapay zekâ ile ilgili 800.000 iş ilanı bulundu. PitchBook araştırmasına göre, 2024'te yeni fon alan girişimlerin %22'si kendilerini yapay zekâ şirketi olarak tanımlıyordu.\n\n\n== Gelişim süreci ==\n\n\n=== İlk araştırmalar ve yapay sinir ağları ===\nİdealize edilmiş tanımıyla yapay zekâ konusundaki ilk çalışmalardan biri McCulloch ve Pitts tarafından yapılmıştır. Bu araştırmacıların önerdiği, yapay sinir hücrelerini kullanan hesaplama modeli, önermeler mantığı, fizyoloji ve Turing'in hesaplama kuramına dayanıyordu. Herhangi bir hesaplanabilir fonksiyonun sinir hücrelerinden oluşan ağlarla hesaplanabileceğini ve mantıksal ve ve veya işlemlerinin gerçekleştirilebileceğini gösterdiler. Bu ağ yapılarının uygun şekilde tanımlanmaları hâlinde öğrenme becerisi kazanabileceğini de ileri sürdüler. Hebb, sinir hücreleri arasındaki bağlantıların şiddetlerini değiştirmek için basit bir kural önerince, öğrenebilen yapay sinir ağlarını gerçekleştirmek de olası hale gelmiştir.\n1950'lerde Shannon ve Turing bilgisayarlar için satranç programları yazıyorlardı. İlk yapay sinir ağı temelli bilgisayar SNARC, MIT'de Minsky ve Edmonds tarafından 1951'de yapıldı. Çalışmalarını Princeton Üniversitesi'nde sürdüren Mc Carthy, Minsky, Shannon ve Rochester'le birlikte 1956 yılında Dartmouth'da iki aylık bir açık çalışma düzenledi. Bu toplantıda birçok çalışmanın temelleri atılmakla birlikte, toplantının en önemli özelliği Mc Carthy tarafından önerilen yapay zekâ adının konmasıdır. İlk kuram ispatlayan programlardan Logic Theorist (Mantık kuramcısı) burada Newell ve Simon tarafından tanıtılmıştır.\n\n\n=== Yeni yaklaşımlar ===\nDaha sonra Newell ve Simon, insan gibi düşünme yaklaşımına göre üretilmiş ilk program olan Genel Sorun Çözücü (General Problem Solver)'ı geliştirmişlerdir. Simon, daha sonra fiziksel simge varsayımını ortaya atmış ve bu kuram, insandan bağımsız zeki sistemler yapma çalışmalarıyla uğraşanların hareket noktasını oluşturmuştur. Simon'ın bu tanımlaması bilim adamlarının yapay zekâya yaklaşımlarında iki farklı akımın ortaya çıktığını belirginleştirmesi açısından önemlidir: Sembolik Yapay Zekâ ve Sibernetik Yapay Zekâ.\n\n\n== Yaklaşımlar ve eleştiriler ==\n\n\n=== Sembolik yapay zekâ ===\nSimon'ın sembolik yaklaşımından sonraki yıllarda mantık temelli çalışmalar egemen olmuş ve programların başarımlarını göstermek için bir takım yapay sorunlar ve dünyalar kullanılmıştır. Daha sonraları bu sorunlar gerçek yaşamı hiçbir şekilde temsil etmeyen oyuncak dünyalar olmakla suçlanmış ve yapay zekânın yalnızca bu alanlarda başarılı olabileceği ve gerçek yaşamdaki sorunların çözümüne ölçeklenemeyeceği ileri sürülmüştür.\nGeliştirilen programların gerçek sorunlarla karşılaşıldığında çok kötü bir başarım göstermesinin ardındaki temel neden, bu programların yalnızca sentaktik süreçleri benzeşimlendirerek anlam çıkarma, bağlantı kurma ve fikir yürütme gibi süreçler konusunda başarısız olmasıydı. Bu dönemin en ünlü programlarından Weizenbaum tarafından geliştirilen Eliza, karşısındaki ile sohbet edebiliyor gibi görünmesine karşın, yalnızca karşısındaki insanın cümleleri üzerinde bazı işlemler yapıyordu. İlk makine çevirisi çalışmaları sırasında benzeri yaklaşımlar kullanılıp çok gülünç çevirilerle karşılaşılınca bu çalışmaların desteklenmesi durdurulmuştu. Bu yetersizlikler aslında insan beynindeki semantik süreçlerin yeterince incelenmemesinden kaynaklanmaktaydı.\n\n\n=== Sibernetik yapay zekâ ===\nYapay sinir ağları çalışmalarının dahil olduğu sibernetik cephede de durum aynıydı. Zeki davranışı benzeşimlendirmek için bu çalışmalarda kullanılan temel yapılardaki bazı önemli yetersizliklerin ortaya konmasıyla birçok araştırmacılar çalışmalarını durdurdular. Buna en temel örnek, Yapay sinir ağları konusundaki çalışmaların Marvin Minsky ve Seymour Papert'in 1969'da yayınlanan Perceptrons adlı kitaplarında tek katmanlı algaçların bazı basit problemleri çözemeyeceğini gösterip aynı kısırlığın çok katmanlı algaçlarda da beklenilmesi gerektiğini söylemeleri ile bıçakla kesilmiş gibi durmasıdır.\nSibernetik akımın uğradığı başarısızlığın temel sebebi de benzer şekilde Yapay Sinir Ağının tek katmanlı görevi başarması fakat bu görevle ilgili vargıların veya sonuçların bir yargıya dönüşerek diğer kavramlar ile bir ilişki kurulamamasından kaynaklanmaktadır. Bu durum aynı zamanda semantik süreçlerin de benzeşimlendirilememesi gerçeğini doğurdu.\n\n\n== Uzman sistemler ==\nHer iki akımın da uğradığı başarısızlıklar, her sorunu çözecek genel amaçlı sistemler yerine belirli bir uzmanlık alanındaki bilgiyle donatılmış programları kullanma fikrinin gelişmesine sebep oldu ve bu durum yapay zekâ alanında yeniden bir canlanmaya yol açtı. Kısa sürede Uzman sistemler adı verilen bir metodoloji gelişti.\nUzman sistemler bir konuda belli ön koşullar aynı anda var olduğunda konunun bir uzmanın (bazen ne olasılıkla) ne karar alacağını belirleyen kuralların tümünü içeren bir programı gelen problemlere uygulamak temellidir. Bunun bir avantajı her verilen kararın hangi kurallar uygulanarak verildiğinin kolayca bilinmesi idi. Bu birçok kuralcı bürokratik karar örgütleri için kolayca uygulamalar geliştirilebilmesi demekti. Bu doğal olarak bir otomobilin tamiri için önerilerde bulunan uzman sistem programının otomobilin ne işe yaradığından haberi olmaması da demekti. Buna rağmen uzman sistemlerin başarıları beraberinde ilk ticari uygulamaları da getirdi.\nYapay zekâ yavaş yavaş bir endüstri hâline geliyordu. DEC tarafından kullanılan ve müşteri siparişlerine göre donanım seçimi yapan R1 adlı uzman sistem şirkete bir yılda 40 milyon dolarlık tasarruf sağlamıştı. Birden diğer ülkeler de yapay zekâyı yeniden keşfettiler ve araştırmalara büyük kaynaklar ayrılmaya başlandı. 1988'de yapay zekâ endüstrisinin cirosu 2 milyar dolara ulaşmıştı.\n\n\n== Doğal dil işleme ==\nAntropoloji bilimi, gelişmiş insan zekâsı ile dil arasındaki bağlantıyı gözler önüne serdiğinde, dil üzerinden yürütülen yapay zekâ çalışmaları tekrar önem kazandı. İnsan zekâsının doğrudan doğruya kavramlarla düşünmediği, dil ile düşündüğü, dil kodları olan kelimeler ile kavramlar arasında bağlantı kurduğu anlaşıldı. Bu sayede insan aklı kavramlar ile düşünen Hayvan beyninden daha hızlı işlem yapabilmekteydi ve dil dizgeleri olan cümleler yani şablonlar ile etkili bir öğrenmeye ve bilgisini soyut olarak genişletebilme yeteneğine sahip olmuştu. İnsanların iletişimde kullandıkları Türkçe, İngilizce gibi doğal dilleri anlayan bilgisayarlar konusundaki çalışmalar hızlanmaya başladı. Önce, yine Uzman sistemler olarak karşımıza çıkan doğal dil anlayan programlar, daha sonra Sembolik Yapay Zekâ ile ilgilenenler arasında ilgiyle karşılandı ve yazılım alanındaki gelişmeler sayesinde İngilizce olan A.I.M.L (Artificial intelligence Markup Language) ve Türkçe T.Y.İ.D (Türkçe Yapay Zekâ İşaretleme Dili) gibi bilgisayar dilleri ile sentaktik (Örüntü) işlemine uygun veri erişim metotları geliştirilebildi. Bugün Sembolik Yapay Zekâ araştırmacıları özel Yapay Zekâ dillerini kullanarak verileri birbiri ile ilişkilendirebilmekte, geliştirilen özel prosedürler sayesinde anlam çıkarma ve çıkarımsama yapma gibi ileri seviye bilişsel fonksiyonları benzetimlendirmeye çalışmaktadırlar.\nBütün bu gelişmelerin ve süreçlerin sonunda bir grup yapay zekâ araştırmacısı, insan gibi düşünebilen sistemleri araştırmaya devam ederken diğer bir grup ise ticari değeri olan rasyonel karar alan sistemler (Uzman sistemler) üzerine yoğunlaştı.\n\n\n=== Diyalog bazlı yapay zeka ===\nDoğal dil işleme ve makine öğrenmesi gibi yapay zeka teknolojileri kullanılarak insan ve makine (yazılım) arasında bir diyaloğun sürdürülmesini sağlayan yapay zeka alt dalına \"diyalog bazlı yapay zeka\" (conversational artificial intelligence) denir. Daha önce insanların bilgisayara komut vermesinde kullanılan web, mobil uygulama gibi grafiksel arayüzlerin (GUI) yerine geçmeyi amaçlayan diyalog bazlı arayüzler (CUI) insanların bilgisayara günlük dilde yazarak veya konuşarak komut verebilmesini amaçlar. Günümüzde, chatbotlar ve sesli asistanlar diyalog bazlı yapay zeka alanında sıkça kullanılan teknolojik ürünler olarak karşımıza çıkmaktadır.\n\n\n==== Chatbotlar ====\nChatbotlar, diyalog bazlı yapay zekanın günlük hayatta kullanılan bir örneğidir. Kullanıcılar, Türkçeye sohbet robotları olarak geçmiş bu dijital ürünler ile yazışarak belirli bir konuda bilgi alabilir veya uçak bileti almak, banka havalesi yapmak veya bir kitap satın almak gibi günlük işlerini yapabilirler. Chatbotlar, şirketlerin web sitesinde veya mobil uygulamasında yer alabilirler. Bunun dışında chatbotlar, WhatsApp, Facebook Messenger gibi genel mesajlaşma platformlarında veya Google Assistant, Siri gibi sesli asistanlarda da yer alabilirler.\nChatbotlar kullanıcı ile etkileşim kurma yöntemini, arkasında yer alan teknolojik altyapıya göre farklı çeşitlerde oluşturulabilir. Örneğin bir chatbot kullanıcı ile, sadece kullanıcı onunla etkileşime girdiğinde iletişim kuruyorsa reaktif bir chatbottur, eğer bir uyarıcı ile tetiklenerek kullanıcı ile olan diyaloğu başlatan taraf oluyorsa buna proaktif bir chatbot denir. Teknoloji açısından bakılacak olursa, yapay zeka tabanlı chatbotların yanında, doğal dil işleme, makine öğrenmesi gibi yapay zeka teknolojileri kullanılmadan geliştirilen kural tabanlı chatbotlar da kullanılmaktadır. Ancak bu iki tür chatbotun davranışı farklıdır. Kural bazlı chatbotlarda genellikle kullanıcıya belirli seçenekler sunulur ve yaratılan deneyim bu seçeneklerle sınırlı kalır. Yapay zeka tabanlı chatbotlarda ise kullanıcı serbest bir metin yazabilir, chatbotun doğal dil işleme teknolojisi bu metni anlamlandırıp doğru yanıtı belirleyerek kullanıcıya sunar.\n\n\n== Gelecekte yapay zekâ ==\n\n\n=== Süper zekâ ve tekillik ===\nSüper zekâ, en parlak ve en yetenekli insan zihninin zekâsını çok aşan bir zekâya sahip varsayımsal bir etkendir. Yapay genel zekâ üzerine yapılan araştırmalar yeterince zeki bir yazılım üretirse, yazılım kendini yeniden programlayabilir ve geliştirebilir. Gelişen yazılım kendini geliştirmede daha da iyi olur ve I. J. Good'un \"zekâ patlaması\" ve Vernor Vinge'in \"tekillik\" olarak adlandırdığı şeye yol açar.\nAncak teknolojiler sonsuza kadar üstel olarak gelişemez ve genellikle S şeklinde bir eğri izler, teknolojinin yapabileceği şeyin fiziksel sınırlarına ulaştığında yavaşlar.\n\n\n=== Transhümanizm ===\n\nRobot tasarımcısı Hans Moravec, sibernetikçi Kevin Warwick ve mucit Ray Kurzweil, gelecekte insanların ve makinelerin birleşerek her ikisinden de daha yetenekli ve güçlü siborglara dönüşebileceğini öngördüler. Transhümanizm olarak adlandırılan bu fikrin kökleri, Aldous Huxley ve Robert Ettinger'ın yazılarında bahsedilmektedir.\nEdward Fredkin, \"yapay zekânın evrimin bir sonraki adımı\" olduğunu savunmaktaydı; bu fikir ilk olarak Samuel Butler'ın \"Darwin among the Machines\" adlı mektubunda 1863'te ortaya atıldı ve George Dyson tarafından 1998'deki Darwin Among the Machines: The Evolution of Global Intelligence adlı kitabında geliştirildi.\n\n\n== Yapay zekânın gücü ==\nBilişim uzmanları, bir insanın hepsi aynı anda paralel olarak çalışan 100 milyar nöron bağlantısının toplam hesap gücünün alt sınırı olan saniyede 10 katrilyon (1.000.000.000.000.000 = \n  \n    \n      \n        \n          10\n          \n            15\n          \n        \n      \n    \n    {\\displaystyle 10^{15}}\n  \n) hesap düzeyine 2025'te erişeceğini düşünüyorlar.\nBeynin bellek kapasitesine gelince, 100 trilyon bağlantının her birine 10.000 bit bilgi depolama gereksinimi tanınırsa, toplam kapasite 10^18 düzeyine çıkıyor. 2020'ye gelindiğinde insan beyninin işlevselliğine erişmiş bir bilgisayarın fiyatının 1000 dolar olacağı tahmin ediliyor. 2030'da 1000 dolarlık bir bilgisayarın bellek kapasitesi 1000 insanın belleğine eşit olacak.\n\n\n== Uygulama alanları ==\nYapay zekanın uygulama alanlarının bazı örnekleri şu şekildedir:\n\nÖnerici sistemler: Kullanıcıların geçmiş davranışlarına dayanarak yeni içerik önerilmesi. Örneğin, sosyal medya sitelerinde yeni arkadaş, mağazalarda başka bir ürün, gazetede başka bir haber önerileri.\nMakine çevirisi: Bir dilde ifade edilen cümleyi farklı bir dile çevirmek. Örneğin, Google Translate, Microsoft Tercüman ve Yandex.Çeviri gibi çevrimiçi araçlar.\nSinyal işleme: Ses ve görüntü gibi sinyallerin işlenerek bilgi çıkarımı. Örneğin, yüz ve ses tanıma.\nProsedürel içerik üretimi: Rassal yöntemler kullanarak yapay içerik üretme. Örneğin, üretimsel müzik ve video oyunlarında prosedürel dünyalar.\nRegresyon analizi: Geçmiş verilere dayanılarak bir değişkenin gelecekteki değerinin tahmin edilmesi. Örneğin, ekonomik öngörüler, üretim miktarı öngörüleri.\nGörüntü işleme: Dijital görüntülerde bulunan objeleri tanıma, yerini bulma, sınıflandırma gibi işlemlerin tümü. Yapay zekadan önce bu işlemler Hough dönüşümü gibi kurala dayalı algoritmalar ile sürdürülürken, günümüzde bu kurallar veriden öğrenilmektedir. Görüntülemenin sık kullanıldığı tıp, biyoloji, otomotiv, üretim gibi alanlarda kullanılmaktadır.\nMakale yazma: Dünyada yapay zeka ile yazılan ilk köşe yazısı 8 Eylül 2020 tarihinde The Guardian gazetesinde yayınlanmıştır. Türkiye'de yapay zekanın yazdığı ilk haber ise Şalom gazetesinde 2018 yılında yayınlanmıştır.\n\n\n== Alt dallar ==\nMakine Zekâsı (Sembolik Yapay Zekâ)\nYapay Sinir Ağları (Sibernetik Yapay Zekâ)\nDoğal Dil işleme (Dil ile düşünme)\nGörüntü İşleme\nKonuşma Sentezi (Yapay Konuşma)\nKonuşma Anlama (Konuşma Analizi)\nTümevarımlı mantık programlama\nBilgi-tabanlı sistem\nBilgisayarlı görü\nDoğal dil üretme\nUzman sistemler\nSinirsel şebeke\nMakine öğrenimi\nÖrüntü Tanıma\nGenetik Algoritmalar\nGenetik Programlama\nBulanık Mantık\nÇoklu Örnekle Öğrenme (Multiple Instance Learning)\n\n\n== Ayrıca bakınız ==\n\n\n== Notlar ==\n\n\n== Kaynakça =="
}