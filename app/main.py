import json
import os
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document
import requests
import re

class TurkishRAGSystem:
    def __init__(self, data_folder_path: str, model_type: str = "deepseek", api_key: str = None):
        """
        TÃ¼rkÃ§e RAG sistemi baÅŸlatÄ±cÄ±sÄ±
        
        Args:
            data_folder_path: JSON dosyalarÄ±nÄ±n bulunduÄŸu klasÃ¶r yolu
            model_type: "deepseek", "openai", "local", "ollama"
            api_key: API anahtarÄ± (deepseek/openai iÃ§in)
        """
        self.data_folder_path = data_folder_path
        self.model_type = model_type
        self.api_key = api_key
        
        # ChromaDB istemcisi
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        
        # TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ embeddings
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'}
        )
        
        # Text splitter - TÃ¼rkÃ§e metinler iÃ§in optimize edilmiÅŸ
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ";", ",", " "]
        )
        
        # Vector store
        self.vector_store = None
        
        # TÃ¼rkÃ§e prompt template
        self.system_prompt = """Sen TÃ¼rkÃ§e konuÅŸan bir yapay zeka asistanÄ±sÄ±n. GÃ¶rentin verilen BAÄLAM bilgilerini kullanarak kullanÄ±cÄ±nÄ±n sorusunu yanÄ±tla.

KURALLAR:
1. Sadece verilen BAÄLAM bilgilerini kullan
2. BaÄŸlamda bilgi yoksa "Bu konuda verilen bilgilerde yeterli detay bulunmuyor" de
3. KÄ±sa ve Ã¶z yanÄ±t ver
4. BaÄŸlamdan doÄŸrudan alÄ±ntÄ± yapabilirsin
5. TÃ¼rkÃ§e yanÄ±t ver

BAÄLAM:
{context}

SORU: {question}

YANITÃN:"""

    def load_json_data(self) -> List[Document]:
        """
        Data klasÃ¶rÃ¼ndeki tÃ¼m JSON dosyalarÄ±nÄ± yÃ¼kler ve Document objelerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        """
        documents = []
        
        if not os.path.exists(self.data_folder_path):
            print(f"âŒ {self.data_folder_path} klasÃ¶rÃ¼ bulunamadÄ±!")
            return documents
        
        # Data klasÃ¶rÃ¼ndeki tÃ¼m JSON dosyalarÄ±nÄ± bul
        json_files = [f for f in os.listdir(self.data_folder_path) if f.endswith('.json')]
        print(f"ğŸ“‚ {len(json_files)} JSON dosyasÄ± bulundu: {json_files}")
        
        for filename in json_files:
            file_path = os.path.join(self.data_folder_path, filename)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    data = json.load(file)
                
                # EÄŸer data bir liste ise, her elemanÄ± iÅŸle
                if isinstance(data, list):
                    for i, item in enumerate(data):
                        doc_content = self._extract_text_from_json(item)
                        if doc_content and len(doc_content.strip()) > 10:
                            doc = Document(
                                page_content=doc_content,
                                metadata={
                                    "source": filename,
                                    "item_index": i,
                                    "file_path": file_path,
                                    "item_count": len(data)
                                }
                            )
                            documents.append(doc)
                
                # EÄŸer data tek bir obje ise
                else:
                    doc_content = self._extract_text_from_json(data)
                    if doc_content and len(doc_content.strip()) > 10:
                        doc = Document(
                            page_content=doc_content,
                            metadata={
                                "source": filename,
                                "file_path": file_path
                            }
                        )
                        documents.append(doc)
                        
            except Exception as e:
                print(f"âŒ {filename} dosyasÄ± yÃ¼klenirken hata: {str(e)}")
        
        print(f"âœ… Toplam {len(documents)} dÃ¶kÃ¼man yÃ¼klendi.")
        return documents

    def _extract_text_from_json(self, json_obj: Dict[str, Any], parent_key: str = "") -> str:
        """
        JSON objesinden metin iÃ§eriÄŸini daha akÄ±llÄ± ÅŸekilde Ã§Ä±karÄ±r
        """
        text_parts = []
        
        def clean_text(text: str) -> str:
            # Gereksiz karakterleri temizle
            text = re.sub(r'[=]{2,}', '', text)
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = text.strip()
            return text
        
        def extract_recursive(obj, current_key=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if isinstance(value, str) and len(value.strip()) > 2:
                        clean_value = clean_text(value)
                        if clean_value:
                            if current_key:
                                text_parts.append(f"{key}: {clean_value}")
                            else:
                                text_parts.append(f"{key}: {clean_value}")
                    elif isinstance(value, (dict, list)):
                        extract_recursive(value, key)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    if isinstance(item, str) and len(item.strip()) > 2:
                        clean_item = clean_text(item)
                        if clean_item:
                            text_parts.append(clean_item)
                    elif isinstance(item, (dict, list)):
                        extract_recursive(item, current_key)
        
        extract_recursive(json_obj, parent_key)
        
        # TekrarlarÄ± kaldÄ±r ve birleÅŸtir
        unique_parts = []
        for part in text_parts:
            if part not in unique_parts and len(part.strip()) > 5:
                unique_parts.append(part)
        
        return "\n".join(unique_parts)

    def create_vector_store(self):
        """
        Vector store oluÅŸturur ve dÃ¶kÃ¼manlarÄ± yÃ¼kler
        """
        print("ğŸ“š JSON veriler yÃ¼kleniyor...")
        documents = self.load_json_data()
        
        if not documents:
            raise ValueError("âŒ HiÃ§ dÃ¶kÃ¼man yÃ¼klenemedi! 'data' klasÃ¶rÃ¼nÃ¼ kontrol edin.")
        
        print("âœ‚ï¸ Metinler parÃ§alara bÃ¶lÃ¼nÃ¼yor...")
        split_docs = self.text_splitter.split_documents(documents)
        
        print(f"ğŸ“„ Toplam {len(split_docs)} metin parÃ§asÄ± oluÅŸturuldu.")
        
        # Mevcut collection'Ä± temizle
        try:
            self.chroma_client.delete_collection("turkish_rag_collection")
        except:
            pass
        
        print("ğŸ—„ï¸ Vector store oluÅŸturuluyor...")
        self.vector_store = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            client=self.chroma_client,
            collection_name="turkish_rag_collection"
        )
        
        print("âœ… Vector store baÅŸarÄ±yla oluÅŸturuldu!")

    def search_relevant_docs(self, question: str, k: int = 5) -> List[Document]:
        """
        Soruya en uygun dÃ¶kÃ¼manlarÄ± bulur
        """
        if not self.vector_store:
            return []
        
        # Similarity search
        docs = self.vector_store.similarity_search(question, k=k)
        return docs

    def call_llm(self, prompt: str) -> str:
        """
        LLM'i Ã§aÄŸÄ±rÄ±r ve yanÄ±t alÄ±r
        """
        if self.model_type == "deepseek" and self.api_key:
            return self._call_deepseek(prompt)
        elif self.model_type == "openai" and self.api_key:
            return self._call_openai(prompt)
        elif self.model_type == "ollama":
            return self._call_ollama(prompt)
        else:
            return self._call_local_llm(prompt)

    def _call_deepseek(self, prompt: str) -> str:
        """
        DeepSeek API Ã§aÄŸrÄ±sÄ±
        """
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1,
                "max_tokens": 500
            }
            
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"âŒ DeepSeek API hatasÄ±: {response.status_code}")
                return f"API hatasÄ±: {response.status_code}"
                
        except Exception as e:
            print(f"âŒ DeepSeek baÄŸlantÄ± hatasÄ±: {str(e)}")
            return "DeepSeek'e baÄŸlanÄ±lamadÄ±. Local model kullanÄ±lÄ±yor."

    def _call_openai(self, prompt: str) -> str:
        """
        OpenAI API Ã§aÄŸrÄ±sÄ±
        """
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1,
                "max_tokens": 500
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"OpenAI API hatasÄ±: {response.status_code}"
                
        except Exception as e:
            return f"OpenAI baÄŸlantÄ± hatasÄ±: {str(e)}"

    def _call_ollama(self, prompt: str) -> str:
        """
        Ollama API Ã§aÄŸrÄ±sÄ±
        """
        try:
            payload = {
                "model": "llama2:7b",
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["response"]
            else:
                return f"Ollama hatasÄ±: {response.status_code}"
                
        except Exception as e:
            return f"Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±sÄ±z: {str(e)}"

    def _call_local_llm(self, prompt: str) -> str:
        """
        Basit local LLM (context-aware)
        """
        # Context ve soruyu ayÄ±r
        if "BAÄLAM:" in prompt and "SORU:" in prompt:
            context_start = prompt.find("BAÄLAM:") + 8
            context_end = prompt.find("SORU:")
            context = prompt[context_start:context_end].strip()
            
            question_start = prompt.find("SORU:") + 5
            question_end = prompt.find("YANITÃN:")
            question = prompt[question_start:question_end].strip()
            
            return self._generate_contextual_answer(context, question)
        
        return "Prompt formatÄ± hatalÄ±."

    def _generate_contextual_answer(self, context: str, question: str) -> str:
        """
        Context'e dayalÄ± akÄ±llÄ± yanÄ±t Ã¼retir
        """
        if not context or len(context.strip()) < 10:
            return "Bu konuda verilen bilgilerde yeterli detay bulunmuyor."
        
        question_lower = question.lower()
        context_lower = context.lower()
        
        # Algoritma sorusu
        if "algoritma" in question_lower:
            if "algoritma" in context_lower:
                # Context'ten algoritma ile ilgili kÄ±smÄ± bul
                lines = context.split('\n')
                relevant_lines = []
                for line in lines:
                    if 'algoritma' in line.lower() or 'algorithm' in line.lower():
                        relevant_lines.append(line)
                        
                if relevant_lines:
                    return f"Algoritma hakkÄ±nda ÅŸu bilgiler bulundu: {' '.join(relevant_lines[:3])}"
                else:
                    # Context'te genel bilgi varsa
                    context_sample = context[:300] + "..." if len(context) > 300 else context
                    return f"Algoritma konusunda doÄŸrudan bilgi bulunamadÄ±, ancak ÅŸu genel bilgiler mevcut: {context_sample}"
            else:
                return "Algoritma konusunda verilen bilgilerde detay bulunmuyor."
        
        # Genel soru tipleri
        if any(word in question_lower for word in ["nedir", "ne"]):
            # TanÄ±m arÄ±yor
            definition_lines = []
            lines = context.split('\n')
            for line in lines:
                if ':' in line and len(line) < 200:
                    definition_lines.append(line)
            
            if definition_lines:
                return f"TanÄ±m: {definition_lines[0]}. {definition_lines[1] if len(definition_lines) > 1 else ''}"
            else:
                context_sample = context[:200] + "..." if len(context) > 200 else context
                return f"Bu konuda ÅŸu bilgiler mevcut: {context_sample}"
        
        elif any(word in question_lower for word in ["hangi", "ne gibi", "nasÄ±l"]):
            # Liste/Ã¶rnekler arÄ±yor
            items = []
            lines = context.split('\n')
            for line in lines:
                line = line.strip()
                if line and (line.startswith('-') or line.startswith('â€¢') or 
                           line.startswith('*') or ':' in line):
                    items.append(line)
            
            if items:
                return f"Åunlar bulundu: {', '.join(items[:5])}"
            else:
                return f"Bu konuda ÅŸu bilgiler mevcut: {context[:250]}..."
        
        elif any(word in question_lower for word in ["Ã¶nemli", "kritik", "temel"]):
            # Ã–nemli bilgiler arÄ±yor
            important_lines = []
            lines = context.split('\n')
            for line in lines:
                if any(keyword in line.lower() for keyword in ['Ã¶nemli', 'temel', 'ana', 'baÅŸlÄ±ca', 'kritik']):
                    important_lines.append(line)
            
            if important_lines:
                return f"Ã–nemli bilgiler: {' '.join(important_lines[:2])}"
            else:
                # Ä°lk birkaÃ§ cÃ¼mleyi Ã¶nemli kabul et
                sentences = context.split('.')[:2]
                return f"Temel bilgiler: {'. '.join(sentences)}."
        
        # VarsayÄ±lan: baÄŸlamÄ±n baÅŸÄ±ndan Ã¶rnekle
        context_lines = context.split('\n')
        relevant_lines = [line for line in context_lines if line.strip() and len(line) > 20][:3]
        
        if relevant_lines:
            return f"Bu konuda ÅŸu bilgiler mevcut: {' '.join(relevant_lines)}"
        else:
            context_sample = context[:300] + "..." if len(context) > 300 else context
            return f"BaÄŸlamda ÅŸu bilgiler var: {context_sample}"

    def ask_question(self, question: str, k: int = 5) -> Dict[str, Any]:
        """
        RAG sistemi ile soru sorar
        
        Args:
            question: TÃ¼rkÃ§e soru
            k: KaÃ§ dÃ¶kÃ¼man getirileceÄŸi
            
        Returns:
            YanÄ±t ve kaynak dÃ¶kÃ¼manlar
        """
        if not self.vector_store:
            return {
                "question": question,
                "answer": "âŒ Vector store henÃ¼z hazÄ±r deÄŸil!",
                "source_documents": [],
                "context_used": ""
            }
        
        try:
            # 1. Ä°lgili dÃ¶kÃ¼manlarÄ± bul
            print(f"ğŸ” '{question}' sorusu iÃ§in dÃ¶kÃ¼manlar aranÄ±yor...")
            relevant_docs = self.search_relevant_docs(question, k=k)
            
            if not relevant_docs:
                return {
                    "question": question,
                    "answer": "Bu soruyla ilgili hiÃ§bir bilgi bulunamadÄ±.",
                    "source_documents": [],
                    "context_used": ""
                }
            
            # 2. Context oluÅŸtur
            context_parts = []
            for i, doc in enumerate(relevant_docs, 1):
                context_parts.append(f"[Kaynak {i} - {doc.metadata.get('source', 'Unknown')}]\n{doc.page_content}")
            
            context = "\n\n".join(context_parts)
            
            # 3. Prompt oluÅŸtur
            full_prompt = self.system_prompt.format(
                context=context,
                question=question
            )
            
            print(f"ğŸ¤– LLM yanÄ±tÄ± Ã¼retiliyor ({self.model_type})...")
            
            # 4. LLM'den yanÄ±t al
            answer = self.call_llm(full_prompt)
            
            # 5. Sonucu dÃ¶ndÃ¼r
            return {
                "question": question,
                "answer": answer,
                "source_documents": [
                    {
                        "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                        "source": doc.metadata.get("source", "Bilinmiyor"),
                        "metadata": doc.metadata
                    }
                    for doc in relevant_docs
                ],
                "context_used": context[:500] + "..." if len(context) > 500 else context
            }
            
        except Exception as e:
            return {
                "question": question,
                "answer": f"âŒ Hata oluÅŸtu: {str(e)}",
                "source_documents": [],
                "context_used": ""
            }

    def initialize_system(self):
        """
        TÃ¼m sistemi baÅŸlatÄ±r
        """
        print("ğŸš€ RAG sistemi baÅŸlatÄ±lÄ±yor...")
        self.create_vector_store()
        print("âœ… RAG sistemi hazÄ±r!")

    def test_system(self):
        """
        Sistemi test eder
        """
        test_questions = [
            "Bu veriler hakkÄ±nda ne biliyorsun?",
            "Algoritma nedir?",
            "Hangi konular ele alÄ±nÄ±yor?",
            "Veri bilimi hakkÄ±nda ne var?",
            "En Ã¶nemli bilgiler nelerdir?"
        ]
        
        print("\n" + "="*60)
        print("ğŸ§ª SÄ°STEM TESTÄ° BAÅLIYOR")
        print("="*60)
        
        for question in test_questions:
            print(f"\nâ“ SORU: {question}")
            print("-" * 40)
            
            result = self.ask_question(question)
            print(f"ğŸ’¬ YANIT: {result['answer']}")
            
            if result['source_documents']:
                print(f"\nğŸ“š KAYNAK SAYISI: {len(result['source_documents'])}")
                for i, doc in enumerate(result['source_documents'][:2], 1):
                    print(f"   {i}. {doc['source']}: {doc['content'][:100]}...")

# KullanÄ±m Ã¶rneÄŸi
def main():
    # KonfigÃ¼rasyon
    DATA_FOLDER = "./data"
    from apiKey import deepseekapikey    
    # Model seÃ§imi
    print("ğŸ¤– Model seÃ§enekleri:")
    print("1. Local (HÄ±zlÄ±, API gerekmez)")
    print("2. DeepSeek (Kaliteli, API key gerekir)")  
    print("3. OpenAI (Kaliteli, API key gerekir)")
    print("4. Ollama (Yerel, Ollama kurulumu gerekir)")
    
    choice = input("SeÃ§iminiz (1-4, varsayÄ±lan: 1): ").strip() or "1"
    
    model_type = "local"
    api_key = None
    
    if choice == "2":
        model_type = "deepseek"
        api_key = deepseekapikey
    elif choice == "3":
        model_type = "openai"  
        api_key = input("OpenAI API key: ").strip()
    elif choice == "4":
        model_type = "ollama"
    
    try:
        # RAG sistemini baÅŸlat
        rag_system = TurkishRAGSystem(
            data_folder_path=DATA_FOLDER,
            model_type=model_type,
            api_key=api_key
        )
        
        rag_system.initialize_system()
        
        # Test Ã§alÄ±ÅŸtÄ±r
        rag_system.test_system()
        
        # Ä°nteraktif mod
        print("\n" + "="*60)
        print("ğŸ’¬ Ä°NTERAKTÄ°F MOD (Ã§Ä±kmak iÃ§in 'exit')")
        print("="*60)
        
        while True:
            question = input("\nâ“ Sorunuz: ").strip()
            if question.lower() in ['exit', 'Ã§Ä±kÄ±ÅŸ', 'quit']:
                break
                
            if question:
                result = rag_system.ask_question(question)
                print(f"\nğŸ’¬ {result['answer']}")
        
    except Exception as e:
        print(f"âŒ Sistem hatasÄ±: {str(e)}")

if __name__ == "__main__":
    main()